# Data Platform

## Overview

Historically, data has been siloed by departments and even within teams in each department which creates issues like multiple variants of important datasets, a decrease in data accuracy, difficulty in cross-department collaboration, and impedes the goal of Boston being a data-driven city. To combat these issues, we are building a [centralized data platform](https://statescoop.com/bostons-new-centralized-data-platform-is-its-starting-point-for-predictive-analytics/) for the entire City of Boston which serves as a single source of truth for its data. This allows each department to maintain ownership over their data but with built-in data best practices, tools, and support from our team. So, far we've onboarded 28 out of our 74 departments and are continuously working on breaking down our city's data silos.

In this guide, you'll find information and tutorials on all of the features of our data platform and how we maintain a strong data foundation that is accurate, reliable, and secure. At a high-level, we use a tool called [Civis Platform](employee-handbook/tools/civis-platform.md) for everything from querying/analyzing data to building automated pipelines, visualizations, and models. We also run our own infrastructure on [AWS](employee-handbook/tools/amazon-web-services.md) for custom applications, visualizations, and emergency situations.

## Querying Data

### **Data Classification**

Before you start querying data, it's good to have an understanding of how data is organized and classified. All data stored in our data warehouse will be classified into one of the below three access level types, in accordance with our Data Governance Policy. 

#### Open data

* No restrictions on internal use by any user of the platform or City of Boston employee. 
* Any data already shared on the [Analyze Boston](open-data.md) portal is classified as open data. However, not all data in open data schemas are published on Analyze Boston, it must first go through a [separate approval process](https://bostonopendata.knack.com/opendataapprovalpublication#processoverview/). All squares are rectangles but not all rectangles are squares.

#### Internal data \(default classification level\)

* No restrictions on internal use by members of the Analytics team.
* Users from other teams must request access to each internal data schema they want, the data owner will approve the request prior to the requesting team being granted access. 

#### Restricted data

* Anyone, including all members of the Analytics team, must request access on a table by table basis from the data owner using the [Data Access Request Form](https://bostonopendata.knack.com/DAR#home/). 

{% hint style="info" %}
An administrator of the Civis Platform can see all the data stored in the platform. By nature, the administrator role is designed to have full access.
{% endhint %}

### Schema Naming Conventions

* All schema names align with the above Data Classification categories. 
* The prefix of each schema designates the department or project to which the data belongs. 
  * Data owned by the Environment department will be held in schemas prefixed with `env_` , and followed by `internal_data`, `open_data`, or `restricted_data` to correspond with access levels outlined above. 
  * Data coming from the 311 system doesn't have a designated department owner outside of DoIT, so it is prefixed with `lagan_311_` to indicate the project name. Same for Hansen/IPS data, it's prefixed with `hansen_` instead of `[department]_hansen_`. 
  * Data owned and generated by the Analytics Team is held in schemas prefixed with `analytics_` . 
  * In addition to the normal schemas, there is a `sandbox` schema meant for ad hoc and testing purposes of the Analytics Team.
* If you are a member of the Analytics Team, you will have full read access to any table in any `*_open_data` and `*_internal_data` schemas. 
* All `restricted_data` tables are on a person by person access only, you must request it for yourself for a specific project. More details [here](). 
* All members of the Analytics Team can write to the sandbox schema. Please read the [Table Naming Guide]() before creating tables.

{% hint style="danger" %}
Any table in `sandbox` is subject to deletion at any time - please do not build visualizations or automation on these tables. Also, storage of restricted data is **not allowed** in the`sandbox` schema.
{% endhint %}

## Analyzing Data

## Importing, Transforming, and Exporting Data

## Data Pipelines

## Naming Conventions

### Schema Naming Conventions

### Table/View Naming Conventions

In general, the table/view naming conventions make it easier to find datasets as well as quickly understand what is in them and what they are used for. Please think carefully about your table name. Don’t use extra filler words, don’t put your name into production tables \(ex: mbta\_alerts\_janedoe\), and make the table name be descriptive of what the purpose of the table is.

**Overall guidelines:**

* All table names should be **lowercase.**
* All table names should use [**snake case**](https://en.wikipedia.org/wiki/Snake_case)**.**

{% hint style="success" %}
\[**high\_level\_project\_name**\]\_\[**purpose\_of\_table**\]\_\[optional: **suffix** \(see below\)\]
{% endhint %}

Putting the high-level project name first will allow all tables to be grouped together and will allow all the related data to be located faster.

**Table/View Suffixes:**

* `_view` or `_vw` **-&gt;** represents this is a view and not a table
* `_staging` or `_stg` **-&gt;** represents a table that is temporarily used for computation

**Good Examples:**

* _mbta\_alerts_ **\(project and purpose are clear\)**
* _mbta\_alerts\_staging_ **\(clear this is a staging table\)**
* _mbta\_alerts\_previous\_24\_hours\_view_ **\(clear this is a view\)**

**Bad Examples:**

* _alerts\_mbta_ **\(project name should go first\)**
* _mbta\_alerts\_new_ **\(\_new implies there are now two versions of truth\)**
* _mbta\_new_ **\(unclear what data the table actually stores\)**
* _mbta\_alerts\_new\_test_ **\(ok in sandbox, never ok in production\)**
* _mbta\_alerts\_2_ **\(there is really never any reason for the \_2\)**

{% hint style="warning" %}
A table in the **sandbox** schema should have your initials prefixed in the table name, ex: a table created in sandbox by Jane Doe containing salaries should be labeled ****`jd_employee_salary`
{% endhint %}

### Column Naming Conventions

**Overall guidelines:**

* All column names should be lowercase.
  * Column names are case sensitive in Postgres.
* All column names should use [**snake case**](https://en.wikipedia.org/wiki/Snake_case)**.**
* Whenever possible and useful, make all the column names identical to the column names in the source data. 
* Column names may only be title case or uppercase in a view used for a report or application.

### Data Pipeline Naming Conventions

Data pipeline definitions live in version control in GitHub but only become live when they are added in platform. This section talks about the naming conventions for both the definition and the in-platform workflow.

#### In GitHub

**Overall guidelines:**

* Group workflows together alphabetically by putting the high level project name first, ex: 311, Hansen.
* Designate the source and target \(import and export\) platforms of the data. 
* Make the workflow name in Civis and the yaml file name in Github as identical as possible to ease lookup.
* All data pipeline definition files should be lowercase.
* All data pipeline definition files should use [**snake case**](https://en.wikipedia.org/wiki/Snake_case)**.**

{% hint style="success" %}
project\_name\_from\_source\[\_source2\]\_to\_destination\[\_destination2\]
{% endhint %}

**Good Examples:**

* covid\_john\_hopkins\_historical\_from\_github\_to\_civis
  * for a workflow that pulls COVID John Hopkins data from Github and imports it to Civis \(one source, one destination example\)
* tapas\_from\_googlesheet\_knack\_to\_arcgis.yaml
  * for a workflow that pulls data from a Google Sheet and a Knack form and uploads it to ArcGIS. \(multiple sources example\)
* mbta\_heatmap\_from\_mbta\_api\_to\_s3
  * for a workflow that pulls data from the MBTA API and uploads it to S3 for the MBTA heatmap project. 

#### In Civis Platform

The name of the workflow in Civis should be the same as the data pipeline definition yaml file name for that workflow, except replacing the underscores with spaces and applying capitalization rules.

**Example:**

* covid\_john\_hopkins\_historical\_from\_github\_to\_civis **-&gt;** COVID John Hopkins Historical from Github to Civis

### Data Unit Test Naming Conventions

The data unit test files located in [civis\_pipelines/test/data unit test/expectations/](https://github.com/CityOfBoston/civis_pipelines/tree/master/tests/data_unit_tests/expectations) should be named after the table they are running the data unit test on.

**Overall guidelines:**

* Name the data unit test after the table name \(including schema\).
* Use a hyphen between the schema/table name \(a . causes parsing issues\).
* All data unit test names should be lowercase.
* All data unit test files should use [**snake case**](https://en.wikipedia.org/wiki/Snake_case) ****\(except the hyphen\).

**Example:**

* analytics\_open\_data.moving\_truck\_permits **-&gt;** analytics\_open\_data-moving\_truck\_permits.json

## Security

### 

